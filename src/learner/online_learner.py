"""
ì˜¨ë¼ì¸ ì ì‘í˜• í•™ìŠµ(Adaptive Learning) ëª¨ë¸.
ì‹¤ì‹œê°„ ì„±ê³¼(P&L)ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì „ëµ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì œì•ˆ.
"""
import asyncio
import os
import random
from typing import Dict, Any, List, Deque
from collections import deque
from .schema import TradeEvent, Prediction, ExecutionResult, TradeParams
from .feature_store import FeatureStore
from .model_registry import ModelRegistry
from .utils import get_logger

logger = get_logger(__name__)


class OnlineLearner:
    """ìê°€ í•™ìŠµ ë° íŒŒë¼ë¯¸í„° íŠœë‹ ì—”ì§„."""

    def __init__(self):
        self.feature_store = FeatureStore()
        self.registry = ModelRegistry()
        self.update_queue = asyncio.Queue()
        self._is_dry_run = os.getenv("DRY_RUN", "False").lower() == "true"
        
        # [í•µì‹¬] ìµœê·¼ 50íšŒ ê±°ë˜ ì„±ê³¼ ë©”ëª¨ë¦¬ (ë‹¨ê¸° ê¸°ì–µ)
        self.recent_pnl: Deque[float] = deque(maxlen=50)
        
        # í˜„ì¬ ì ìš© ì¤‘ì¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì´ˆê¸°ê°’)
        self.current_params = TradeParams(
            k=0.5, 
            rsi_buy_threshold=25,
            stop_loss_pct=0.005,
            take_profit_pct=0.012,
            volume_multiplier=2.0
        )
        
        # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë£¨í”„ ì‹œì‘
        asyncio.create_task(self._training_loop())

    async def predict(self, event: TradeEvent) -> Prediction:
        """í˜„ì¬ ì‹œì¥ ìƒí™©ê³¼ ê³¼ê±° ì„±ê³¼ë¥¼ ë°˜ì˜í•œ ìµœì  íŒŒë¼ë¯¸í„° ì œì•ˆ."""
        # 1. í”¼ì²˜ ê³„ì‚° (ìƒëµ ê°€ëŠ¥í•˜ë‚˜ í™•ì¥ì„± ìœ„í•´ ìœ ì§€)
        # features = await self.feature_store.compute_features(event)
        
        # 2. ì ì‘í˜• íŒŒë¼ë¯¸í„° ê³„ì‚° (Adaptive Logic)
        adjusted_params = self._adjust_params_based_on_performance()
        
        return Prediction(
            model_version="adaptive_v1",
            suggested_params=adjusted_params,
            estimated_slippage=0.001, # ê³ ì •ê°’ ë˜ëŠ” ì˜ˆì¸¡ê°’
            confidence_score=self._calculate_confidence()
        )

    def _adjust_params_based_on_performance(self) -> TradeParams:
        """ìµœê·¼ ì„±ê³¼ì— ë”°ë¼ ì „ëµ íŒŒë¼ë¯¸í„° ë™ì  íŠœë‹."""
        if not self.recent_pnl:
            return self.current_params # ë°ì´í„° ì—†ìœ¼ë©´ ì´ˆê¸°ê°’ ìœ ì§€

        avg_pnl = sum(self.recent_pnl) / len(self.recent_pnl)
        win_rate = len([p for p in self.recent_pnl if p > 0]) / len(self.recent_pnl)
        
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ë³µì‚¬
        new_params = self.current_params.model_copy()

        # [íŠœë‹ ë¡œì§ 1] ì„±ê³¼ê°€ ì €ì¡°í•  ë•Œ (ìŠ¹ë¥  40% ë¯¸ë§Œ or í‰ê·  ì†ì‹¤) -> ë³´ìˆ˜ì  ëŒ€ì‘
        if win_rate < 0.4 or avg_pnl < 0:
            logger.info(f"ğŸ“‰ ì„±ê³¼ ì €ì¡° (ìŠ¹ë¥ : {win_rate:.2%}). ì§„ì… ì¥ë²½ ê°•í™”.")
            new_params.k = min(0.9, new_params.k + 0.05)           # Kê°’ ìƒí–¥ (ë” í™•ì‹¤í•œ ëŒíŒŒë§Œ)
            new_params.rsi_buy_threshold = max(15, new_params.rsi_buy_threshold - 2) # RSI í•˜í–¥ (ë” ì‹¬í•œ ê³¼ë§¤ë„ë§Œ)
            new_params.volume_multiplier = min(4.0, new_params.volume_multiplier + 0.2) # ê±°ë˜ëŸ‰ ì¡°ê±´ ê°•í™”
            
        # [íŠœë‹ ë¡œì§ 2] ì„±ê³¼ê°€ ìš°ìˆ˜í•  ë•Œ (ìŠ¹ë¥  60% ì´ìƒ) -> ì ê·¹ì  ëŒ€ì‘
        elif win_rate > 0.6 and avg_pnl > 0.005: # í‰ê·  0.5% ì´ìƒ ìˆ˜ìµ
            logger.info(f"ğŸ“ˆ ì„±ê³¼ ìš°ìˆ˜ (ìŠ¹ë¥ : {win_rate:.2%}). ê¸°íšŒ í™•ëŒ€.")
            new_params.k = max(0.3, new_params.k - 0.02)           # Kê°’ í•˜í–¥ (ì§„ì… ì‰½ê²Œ)
            new_params.rsi_buy_threshold = min(35, new_params.rsi_buy_threshold + 1) # RSI ìƒí–¥
            new_params.volume_multiplier = max(1.2, new_params.volume_multiplier - 0.1) # ê±°ë˜ëŸ‰ ì¡°ê±´ ì™„í™”

        return new_params

    def _calculate_confidence(self) -> float:
        """í˜„ì¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ (ìµœê·¼ ìŠ¹ë¥  ê¸°ë°˜)."""
        if not self.recent_pnl: return 0.5
        win_rate = len([p for p in self.recent_pnl if p > 0]) / len(self.recent_pnl)
        return win_rate

    async def feedback(self, result: ExecutionResult):
        """ê±°ë˜ ê²°ê³¼ ìˆ˜ì‹  ë° í•™ìŠµ í ì¶”ê°€."""
        await self.update_queue.put(result)

    async def _training_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„±ê³¼ ë°ì´í„° í•™ìŠµ."""
        logger.info("Adaptive Learning Loop Started.")
        while True:
            try:
                result = await self.update_queue.get()
                
                # ê²°ê³¼ ê¸°ë¡ (í•™ìŠµ)
                pnl = result.pnl_pct
                self.recent_pnl.append(pnl)
                
                logger.info(f"ğŸ“ í•™ìŠµ ì™„ë£Œ: PnL {pnl*100:.2f}% (ìµœê·¼ {len(self.recent_pnl)}íšŒ í‰ê· : {sum(self.recent_pnl)/len(self.recent_pnl)*100:.2f}%)")
                
                self.update_queue.task_done()
            except Exception as e:
                logger.error(f"Learning loop error: {e}")
