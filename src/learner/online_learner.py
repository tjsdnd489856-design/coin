"""
ì˜¨ë¼ì¸ ì ì‘í˜• í•™ìŠµ(Adaptive Learning) ëª¨ë¸.
ì‹¤ì‹œê°„ ì„±ê³¼(P&L)ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì „ëµ íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ì œì•ˆ.
"""
import asyncio
import os
import random
from typing import Dict, Any, List, Deque
from collections import deque
from .schema import TradeEvent, Prediction, ExecutionResult, TradeParams
from .feature_store import FeatureStore
from .model_registry import ModelRegistry
from .utils import get_logger

logger = get_logger(__name__)


class OnlineLearner:
    """ìê°€ í•™ìŠµ ë° íŒŒë¼ë¯¸í„° íŠœë‹ ì—”ì§„."""

    def __init__(self):
        self.feature_store = FeatureStore()
        self.registry = ModelRegistry()
        self.update_queue = asyncio.Queue()
        self._is_dry_run = os.getenv("DRY_RUN", "False").lower() == "true"
        
        # [í•µì‹¬] ìµœê·¼ 50íšŒ ê±°ë˜ ì„±ê³¼ ë©”ëª¨ë¦¬ (ë‹¨ê¸° ê¸°ì–µ)
        self.recent_pnl: Deque[float] = deque(maxlen=50)
        
        # í˜„ì¬ ì ìš© ì¤‘ì¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì´ˆê¸°ê°’)
        self.current_params = TradeParams(
            k=0.5, 
            rsi_buy_threshold=30,
            stop_loss_pct=0.005,
            take_profit_pct=0.015,
            volume_multiplier=1.3
        )
        
        # ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ë£¨í”„ ì‹œì‘
        asyncio.create_task(self._training_loop())

    async def predict(self, event: TradeEvent) -> Prediction:
        """í˜„ì¬ ì‹œì¥ ìƒí™©ê³¼ ê³¼ê±° ì„±ê³¼ë¥¼ ë°˜ì˜í•œ ìµœì  íŒŒë¼ë¯¸í„° ì œì•ˆ."""
        # 1. í”¼ì²˜ ê³„ì‚° (ìƒëµ ê°€ëŠ¥í•˜ë‚˜ í™•ì¥ì„± ìœ„í•´ ìœ ì§€)
        # features = await self.feature_store.compute_features(event)
        
        # 2. ì ì‘í˜• íŒŒë¼ë¯¸í„° ê³„ì‚° (Adaptive Logic)
        adjusted_params = self._adjust_params_based_on_performance()
        
        return Prediction(
            model_version="adaptive_v1",
            suggested_params=adjusted_params,
            estimated_slippage=0.001, # ê³ ì •ê°’ ë˜ëŠ” ì˜ˆì¸¡ê°’
            confidence_score=self._calculate_confidence()
        )

    def _adjust_params_based_on_performance(self) -> TradeParams:
        """ìµœê·¼ ì„±ê³¼(ìŠ¹ë¥ , ì†ìµë¹„, ê¸°ëŒ€ê°’)ì— ë”°ë¼ ì „ëµ íŒŒë¼ë¯¸í„° ë™ì  íŠœë‹."""
        if not self.recent_pnl:
            return self.current_params

        profits = [p for p in self.recent_pnl if p > 0]
        losses = [p for p in self.recent_pnl if p <= 0]
        
        win_rate = len(profits) / len(self.recent_pnl)
        avg_profit = sum(profits) / len(profits) if profits else 0
        avg_loss = abs(sum(losses) / len(losses)) if losses else 0.001
        
        profit_factor = (sum(profits) / abs(sum(losses))) if losses and sum(losses) != 0 else 2.0
        expected_value = (win_rate * avg_profit) - ((1 - win_rate) * avg_loss)
        
        new_params = self.current_params.model_copy()

        # [íŠœë‹ ë¡œì§ 1] ê¸°ëŒ€ê°’ì´ ìŒìˆ˜ì´ê±°ë‚˜ ì†ìµë¹„ê°€ 1.0 ë¯¸ë§Œ (ì†ì‹¤ êµ¬ê°„)
        if expected_value < 0 or profit_factor < 1.1:
            logger.debug(f"ğŸ“‰ ì„±ê³¼ ì €ì¡° (EV: {expected_value:.4f}, PF: {profit_factor:.2f}). ë³´ìˆ˜ì  ì„¤ì • ì ìš©.")
            new_params.k = min(0.85, new_params.k + 0.05)
            new_params.rsi_buy_threshold = max(20, new_params.rsi_buy_threshold - 2)
            new_params.volume_multiplier = min(1.8, new_params.volume_multiplier + 0.1)
            # ì†ì ˆì€ ë” ì§§ê²Œ, ìµì ˆì€ ë” ê¸¸ê²Œ (ì†ìµë¹„ ê°œì„  ì‹œë„)
            new_params.stop_loss_pct = max(0.005, new_params.stop_loss_pct - 0.001)
            
        # [íŠœë‹ ë¡œì§ 2] ì„±ê³¼ ìš°ìˆ˜ (ì†ìµë¹„ 1.5 ì´ìƒ, ê¸°ëŒ€ê°’ ì–‘ìˆ˜)
        elif profit_factor > 1.5 and expected_value > 0.002:
            logger.debug(f"ğŸ“ˆ ì„±ê³¼ ìš°ìˆ˜ (PF: {profit_factor:.2f}). ê¸°íšŒ í™•ëŒ€.")
            new_params.k = max(0.35, new_params.k - 0.03)
            new_params.rsi_buy_threshold = min(35, new_params.rsi_buy_threshold + 2)
            new_params.volume_multiplier = max(1.5, new_params.volume_multiplier - 0.2)

        return new_params

    def _calculate_confidence(self) -> float:
        """í˜„ì¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ (ìµœê·¼ ìŠ¹ë¥  ê¸°ë°˜)."""
        if not self.recent_pnl: return 0.5
        win_rate = len([p for p in self.recent_pnl if p > 0]) / len(self.recent_pnl)
        return win_rate

    async def feedback(self, result: ExecutionResult):
        """ê±°ë˜ ê²°ê³¼ ìˆ˜ì‹  ë° í•™ìŠµ í ì¶”ê°€."""
        await self.update_queue.put(result)

    async def _training_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„±ê³¼ ë°ì´í„° í•™ìŠµ."""
        logger.info("Adaptive Learning Loop Started.")
        while True:
            try:
                result = await self.update_queue.get()
                
                # ê²°ê³¼ ê¸°ë¡ (í•™ìŠµ)
                pnl = result.pnl_pct
                self.recent_pnl.append(pnl)
                
                logger.info(f"ğŸ“ í•™ìŠµ ì™„ë£Œ: PnL {pnl*100:.2f}% (ìµœê·¼ {len(self.recent_pnl)}íšŒ í‰ê· : {sum(self.recent_pnl)/len(self.recent_pnl)*100:.2f}%)")
                
                self.update_queue.task_done()
            except Exception as e:
                logger.error(f"Learning loop error: {e}")
